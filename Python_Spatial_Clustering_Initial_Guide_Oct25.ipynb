{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA-_IzoV3jmP"
      },
      "source": [
        "# Python for Spatial Analysis - Spatial Clustering\n",
        "### Second part of the module of GG3209 Spatial Analysis with GIS.\n",
        "---\n",
        "Dr Fernando Benitez -  University of St Andrews - School of Geography and Sustainable Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwMgw_C73jmQ"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "We reach to the final lecture of this short introducction to python. To finish I would like to present two methods that have been relevant to the spatial analysys field.\n",
        "\n",
        "## K-means and DBSCAN in Spatial Data Analysis\n",
        "\n",
        "One of the main reasons for us to lead this module is to describe you why the Spatial data analysis plays an important role in understanding patterns, trends, and relationships within geographical datasets. As we mentioned in the lecture there are two popular clustering algorithms, K-means and DBSCAN, are widely used in spatial data analysis for uncovering meaningful insights. Let's explore what each algorithm describe and discuss their relevance in the field.\n",
        "\n",
        "### K-means Clustering:\n",
        "\n",
        "It is a *partitioning* method that aims to group data points into 'k' clusters based on similarity (did you recall the Tobler's law?). It operates by iteratively assigning data points to clusters and updating cluster centroids until convergence. K-means is particularly useful for spatial data analysis as it helps identify spatial patterns and groupings within a dataset. It is widely applied in fields such as urban planning, crime analysis, and environmental science.\n",
        "\n",
        "In the context of spatial analysis, K-means can assist in identifying areas of similar characteristics, such as concentrations of certain activities or the segmentation of regions based on specific features. The method can be also be applied to multidimensional datasets and is used to applied to an introduction to spatial clustering. There are pros and cons that comes with this method, that will be covered in the advance module.\n",
        "\n",
        "### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
        "\n",
        "It is a *density-based* clustering algorithm that identifies clusters based on the density of data points in space. Unlike K-means, DBSCAN can discover clusters of arbitrary shapes and sizes. It is especially valuable in spatial data analysis because it can detect clusters in regions with varying data point densities.\n",
        "\n",
        "In the spatial analysis field, DBSCAN is often used to find spatial hotspots, anomalies, or areas with varying population densities. It can be effective in identifying spatial patterns that may not conform to standard geometrical shapes.\n",
        "\n",
        "It is a good practice to try out both methods and start to compare both clustering to define where are the best results.\n",
        "\n",
        "## Why these methods are relevant in the Spatial Data Analysis field.\n",
        "\n",
        "Both K-means and DBSCAN are indispensable tools in the spatial data analyst's toolkit, lets enumerate three initial use cases:\n",
        "\n",
        "- **Pattern Recognition:** These algorithms help recognize spatial patterns, which can be vital for urban planning, environmental monitoring, and resource allocation.\n",
        "\n",
        "- **Anomaly Detection:** They can identify outliers and anomalies in spatial datasets, enabling the detection of irregularities or unexpected phenomena.\n",
        "\n",
        "- **Segmentation:** Clustering aids in segmenting spatial regions based on certain characteristics, facilitating targeted analysis and decision-making.\n",
        "\n",
        "In this notebook, we will implement **K-means** and **DBSCAN** on synthetic and real-world datasets, providing hands-on experience with spatial clustering techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cyv67K2cuzr"
      },
      "source": [
        "# Mount your Drive\n",
        "\n",
        "As we need to access our Google Drive unit, to get access to the data we previously upload., we need to mount that drive as part of this current python session. Run the following code to allow this notebook access to your drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8aeI-fccxor"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQU7LGR0kdNp"
      },
      "source": [
        "# Installing aditional libraries\n",
        "\n",
        "There are some new libraries we need to install to before we jump into the examples. [Lonboard](https://developmentseed.org/lonboard/latest/) will help us to represent big datasets (handles more than a millon of records). As this is a practice we will work with a partially small dataset from the reported crimes in London."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQcqTOxk8oKs"
      },
      "outputs": [],
      "source": [
        "pip install lonboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_A1oDiCzI5s"
      },
      "source": [
        "# Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srUjPod03jmR"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "# This is a super popular library for working with ML and DL algorithms.\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "import shapely\n",
        "import folium\n",
        "import seaborn as sns\n",
        "from lonboard import Map, ScatterplotLayer, viz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc4wfBZniRPH"
      },
      "source": [
        "# K-Means - manual implementation\n",
        "\n",
        "Let's start with a basic example using synthetic data (like fake data we can juts use for testing) and just python specifically numpy, to run the K-Means algoritmn from scratch, step by step. We will create a several functions (now you are familiar with functions in python and numpy). The proces is the following:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP5F5ghmS9-C"
      },
      "source": [
        "**Function No 1** - `initialize_centroids`: This function initializes the centroids for the K-means algorithm.\n",
        "\n",
        "**Input:**\n",
        "- data: The input data points.\n",
        "- k: The number of clusters.\n",
        "\n",
        "**Implementation:**\n",
        "Randomly selects `k `indices from the data without replacement.\n",
        "Returns the corresponding data points as initial centroids. As an example you can think we can manually select any first 4 values from the synt data.\n",
        "Returns the initial centroids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dkhv5sQl3yu6"
      },
      "outputs": [],
      "source": [
        "def initialize_centroids(data, k):\n",
        "    # Randomly select k data points as initial centroids\n",
        "    indices = np.random.choice(len(data), k, replace=False)\n",
        "    centroids = data[indices]\n",
        "    return centroids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyrS0zCQT2JZ"
      },
      "source": [
        "**Function No 2**  `assign_to_clusters`:   This function assigns each data point to the cluster of the nearest centroid.\n",
        "\n",
        "**Input:**\n",
        "- data: The input data points.\n",
        "- centroids: The current centroids.\n",
        "\n",
        "**Implementation:**\n",
        "\n",
        "Computes the *Euclidean distances* from each data point to each centroid.\n",
        "Assigns each data point to the cluster of the nearest centroid.\n",
        "\n",
        "***Returns*** an array of cluster assignments.\n",
        "\n",
        "As suggestion, you can plot what returns, would you be able to make it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaHzii7dT1n0"
      },
      "outputs": [],
      "source": [
        "def assign_to_clusters(data, centroids):\n",
        "    # Compute distances from each data point to each centroid, linalg is a popular way to do it, find out more in the numpy documentation.\n",
        "    distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n",
        "\n",
        "    # Assign each data point to the cluster of the nearest centroid\n",
        "    clusters = np.argmin(distances, axis=1)\n",
        "    return clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apdDtdb2VP__"
      },
      "source": [
        "**Function 3:** - `update_centroids` : This function updates the centroids based on the **mean** of data points in each cluster.\n",
        "\n",
        "**Input:**\n",
        "- data: The input data points.\n",
        "- clusters: The current cluster assignments.\n",
        "- k: The number of clusters.\n",
        "\n",
        "**Implementation:**\n",
        "\n",
        "Iterates over each cluster.\n",
        "Computes the mean of data points in each cluster to get the new centroid.\n",
        "\n",
        "**Returns** an array of updated centroids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJIEg-VBVPw3"
      },
      "outputs": [],
      "source": [
        "def update_centroids(data, clusters, k):\n",
        "    # Update centroids based on the mean of data points in each cluster\n",
        "    centroids = np.array([data[clusters == i].mean(axis=0) for i in range(k)])\n",
        "    return centroids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slv4oGp1WgIs"
      },
      "source": [
        "**Function 4** `k_means`: The main K-means algorithm function, which iteratively initializes centroids, assigns data points to clusters, and updates centroids until convergence or reaching a maximum number of iterations.\n",
        "\n",
        "**Input:**\n",
        "\n",
        "- data: The input data points.\n",
        "- k: The number of clusters.\n",
        "- max_iterations: Maximum number of iterations (default is 100).\n",
        "\n",
        "**Implementation:**\n",
        "Initializes centroids using initialize_centroids.\n",
        "\n",
        "Iterates through the main steps of K-means (assigning to clusters and updating centroids) until convergence or reaching the maximum number of iterations.\n",
        "\n",
        "**Returns** the final cluster assignments and centroids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh-7S2gNW5i5"
      },
      "outputs": [],
      "source": [
        "def k_means(data, k, max_iterations=100):\n",
        "    # Step 1: Initialize centroids\n",
        "    centroids = initialize_centroids(data, k) #See how I can call a function inside another one.\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        # Step 2: Assign data points to clusters\n",
        "        clusters = assign_to_clusters(data, centroids) # again in here.\n",
        "\n",
        "        # Step 3: Update centroids\n",
        "        new_centroids = update_centroids(data, clusters, k)\n",
        "\n",
        "        # Check for convergence (Again optional but just in case)\n",
        "        if np.all(centroids == new_centroids):\n",
        "            break\n",
        "\n",
        "        centroids = new_centroids #Is this part of the loop?, what am I doing here, can you describe?\n",
        "\n",
        "\n",
        "    return clusters, centroids\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS2ptbS4Xkv0"
      },
      "source": [
        "This part of the code generates synthetic data for demonstration purposes, runs the K-means algorithm with k=3, and visualizes the results.\n",
        "\n",
        "**Implementation:**\n",
        "\n",
        "- Concatenates three sets of samples from a normal distribution to create\n",
        "synthetic data.\n",
        "- Calls the k_means function to obtain cluster assignments and final centroids.\n",
        "- Plots the data points, colored by cluster, and highlights the final centroids.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYett_q5Xnce"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic data for demonstration\n",
        "np.random.seed(11)\n",
        "data = np.concatenate([np.random.normal(loc=0, scale=1, size=(150, 2)),\n",
        "                       np.random.normal(loc=5, scale=1, size=(150, 2)),\n",
        "                       np.random.normal(loc=10, scale=1, size=(150, 2))])\n",
        "\n",
        "# Run K-means algorithm with k=3\n",
        "k = 3\n",
        "clusters, final_centroids = k_means(data, k)\n",
        "\n",
        "# Visualize the results\n",
        "plt.scatter(data[:, 0], data[:, 1], c=clusters, cmap='viridis', alpha=0.7)\n",
        "plt.scatter(final_centroids[:, 0], final_centroids[:, 1], c='red', marker='X', label='Centroids')\n",
        "plt.title('K-means Clustering - Manual implementation')\n",
        "plt.xlabel('X Axis')\n",
        "plt.ylabel('Y Axis')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kdhkk7k-O6W"
      },
      "source": [
        "Please now run the code, and experiment with different parameters, and observe how the algorithm converges to cluster the synthetic data.\n",
        "\n",
        "You can start exploring the impact of changing the number of clusters (**`k`**) and the number of iterations (**`max_iterations`**). to see how the plot, clusters and centroids are updated. Feel free to add more code cell above so you can plot and expore all the changes you want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIQRHlASYHou"
      },
      "source": [
        "# K-Means implementation using the sklearn library\n",
        "---\n",
        "Even thougth the code implemented above isn't complicated, and works well for synth data (small) and educational propuses, you don't want to write this amount of code when you want to process big and multidimensional datasets.\n",
        "\n",
        "Thankfully ðŸ¤Ÿ we have the **sklearn** library, now let's see how easy is to run K-Means in just a few steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prdQ3e8PirRB"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "x = np.random.rand(300) * 10\n",
        "y = np.random.rand(300) * 10\n",
        "data = pd.DataFrame({'X': x, 'Y': y})\n",
        "\n",
        "# Create GeoDataFrame from the synthetic data. Now in this case we will GeoPandas., notice we have not defined any CRS, but that's ok.\n",
        "geometry = gpd.points_from_xy(data['X'], data['Y'])\n",
        "gdf = gpd.GeoDataFrame(data, geometry=geometry)\n",
        "\n",
        "# Visualize the synthetic data\n",
        "gdf.plot(marker='p', color='blue', figsize=(8, 8))\n",
        "plt.title('Synthetic Spatial Data')\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Y-axis')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI8uXCHFi9P1"
      },
      "source": [
        "Please address the following questions after having a look at the code from the previous cell.\n",
        "\n",
        "1. What is the purpose of **`np.random.seed(42)`**?\n",
        "\n",
        "2. What is the role of **`gpd.points_from_xy`**?\n",
        "\n",
        "3. What does **`gdf.plot(marker='o', color='blue')`** do?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_82Qd-xjPPq"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Write your responses here ( `edit this cell`):\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJWUA-Cmjjm0"
      },
      "outputs": [],
      "source": [
        "# Implement K-means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "gdf['kmeans_cluster'] = kmeans.fit_predict(gdf[['X', 'Y']])\n",
        "\n",
        "# Visualize K-means clustering result\n",
        "gdf.plot(column='kmeans_cluster', categorical=True, legend=True, figsize=(8, 8), cmap='Set1')\n",
        "\n",
        "# Plot the centroids as a white X\n",
        "centroids = kmeans.cluster_centers_\n",
        "plt.scatter(\n",
        "    centroids[:, 0],\n",
        "    centroids[:, 1],\n",
        "    marker=\"x\",\n",
        "    s=169,\n",
        "    linewidths=3,\n",
        "    color=\"b\",\n",
        "    zorder=10,\n",
        ")\n",
        "plt.title('K-means Clustering Result')\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Y-axis')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gGEfhsKkD4N"
      },
      "source": [
        "After exploring how easy is to implement **K-Means using this popular ML library**. Could you explain the code above and answer the following questions:\n",
        "\n",
        "1. What is the purpose of **`KMeans(n_clusters=3, random_state=42)`**?\n",
        "2. What does **`kmeans.fit_predict(gdf[['X', 'Y']])`** do?\n",
        "3. What does **`gdf.plot(column='kmeans_cluster', categorical=True, legend=True, cmap='Set1')`** do?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJqIlNT5hw6T"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Write your responses here:\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkYqMC4wdmmS"
      },
      "source": [
        "# DBSCAN manual implementation\n",
        "\n",
        "Now it's the turn for the DBSCAN full implementation, read carefully the code and the description of each function to dive into how the data is being processed, we will also create a set of synthetic data and a set of multiple functions to help you understand what is happeing behind the cortains when you apply this clustering method.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWAqbmzYeCAd"
      },
      "source": [
        "**Function 1:** `find_neighbors`:  This function finds neighbors of a given data point within a specified distance (epsilon).\n",
        "\n",
        "**Input:**\n",
        "- data: The input data points.\n",
        "- point_index: Index of the data point for which neighbors are to be found.\n",
        "- epsilon: Maximum distance to consider for neighbors.\n",
        "\n",
        "**Implementation:**\n",
        "\n",
        "Computes distances from the given point to all other data points.\n",
        "Selects indices of data points within the specified distance (epsilon).\n",
        "\n",
        "**Returns** the indices of neighboring points.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MevYFb2QeF7s"
      },
      "outputs": [],
      "source": [
        "def find_neighbors(data, point_index, epsilon):\n",
        "    # Find indices of data points within epsilon distance from the given point\n",
        "    distances = np.linalg.norm(data - data[point_index], axis=1)\n",
        "    neighbors = np.where(distances <= epsilon)[0]\n",
        "    return neighbors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVKWsYoyeWJB"
      },
      "source": [
        "**Function 2:** `expand_cluster`: This function is sligthly more complex that the one we have integrated before the function expands a cluster by assigning points to the current cluster and recursively exploring their neighbors.\n",
        "\n",
        "**Input:**\n",
        "- data: The input data points.\n",
        "- point_index: Index of the current data point.\n",
        "- neighbors: Indices of neighbors of the current data point.\n",
        "- cluster_id: Identifier for the current cluster.\n",
        "- epsilon: Maximum distance to consider for neighbors.\n",
        "- min_samples: Minimum number of neighbors to consider a point a core point.\n",
        "- clusters: Array indicating the cluster assignment of each data point.\n",
        "- visited: Array indicating whether a data point has been visited.\n",
        "\n",
        "**Implementation:**\n",
        "\n",
        "Initialy we need to assigns the current point to the current cluster., then we need to expands the cluster by recursively exploring neighbors and their neighbors., finally we must check if each neighbor has enough neighbors to be considered a core point., if not we dismiss that iteration., As this method include noise, so the last step is to assigns each unassigned neighbor to the current cluster (most likely to become noise)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4dPd7oWizK6"
      },
      "outputs": [],
      "source": [
        "def expand_cluster(data, point_index, neighbors, cluster_id, epsilon, min_samples, clusters, visited):\n",
        "    # Assign the point to the current cluster\n",
        "    clusters[point_index] = cluster_id\n",
        "\n",
        "    # Expand the cluster by iterating over neighbors\n",
        "    for neighbor_index in neighbors:\n",
        "        if not visited[neighbor_index]:\n",
        "            visited[neighbor_index] = True\n",
        "            new_neighbors = find_neighbors(data, neighbor_index, epsilon)\n",
        "\n",
        "            # Check if the neighbor has enough neighbors to be a core point\n",
        "            if len(new_neighbors) >= min_samples:\n",
        "                neighbors = np.union1d(neighbors, new_neighbors)\n",
        "\n",
        "        # Assign the neighbor to the current cluster if not assigned to any cluster\n",
        "        if clusters[neighbor_index] == -1:\n",
        "            clusters[neighbor_index] = cluster_id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoQKg4QIkFOa"
      },
      "source": [
        "**Function 3:** `dbscan`: We have come to the place where we need to call the created functions, here we are defining the main DBSCAN algorithm function, which iterates through data points, expands clusters, and assigns points to clusters.\n",
        "\n",
        "**Input:**\n",
        "- data: The input data points.\n",
        "- epsilon: Maximum distance to consider for neighbors.\n",
        "- min_samples: Minimum number of neighbors to consider a point a core point.\n",
        "\n",
        "**Implementation:**\n",
        "\n",
        "Initializes variables for cluster assignments and visited points. Notice how this methods wont require a defined number of clusters. Even though both methods are classifed as unsupervised clustering methods, as you don't need any traning dataset to define the size and shape of clusters., then the funcion will iterates through each data point and expands clusters using `expand_cluster`.\n",
        "\n",
        "**Returns** the array of cluster assignments. Think about the categories or label from each cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zby8X-6nkGTW"
      },
      "outputs": [],
      "source": [
        "def dbscan(data, epsilon, min_samples):\n",
        "    # Initialize variables\n",
        "    num_points = len(data)\n",
        "    clusters = np.full(num_points, -1)  # -1 represents unassigned points\n",
        "    visited = np.full(num_points, False)\n",
        "\n",
        "    # Initialize cluster ID\n",
        "    cluster_id = 0\n",
        "\n",
        "    for point_index in range(num_points):\n",
        "        if not visited[point_index]:\n",
        "            visited[point_index] = True\n",
        "            neighbors = find_neighbors(data, point_index, epsilon)\n",
        "\n",
        "            # Check if the point is a core point\n",
        "            if len(neighbors) >= min_samples:\n",
        "                expand_cluster(data, point_index, neighbors, cluster_id, epsilon, min_samples, clusters, visited)\n",
        "                cluster_id += 1\n",
        "\n",
        "    return clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIubOqJDk_1f"
      },
      "source": [
        "**Putting all together and plot**\n",
        "\n",
        "Finally the code bellow will generates synthetic data for demonstration purposes, we will use the same methods we run previously, then runs the DBSCAN algorithm, and visualizes the results using the `matplotlib`. Please feel free to add new code cell and plot the intermediate results, or data to get a sense of how the data is being manipulated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhiGaBGylAPz"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic data for demonstration.\n",
        "\n",
        "np.random.seed(42)\n",
        "data = np.concatenate([np.random.normal(loc=0, scale=1, size=(100, 2)),\n",
        "                       np.random.normal(loc=5, scale=1, size=(100, 2)),\n",
        "                       np.random.normal(loc=10, scale=1, size=(100, 2))])\n",
        "\n",
        "# Run DBSCAN algorithm with epsilon=1 and min_samples=5\n",
        "epsilon = 1\n",
        "min_samples = 5\n",
        "clusters = dbscan(data, epsilon, min_samples)\n",
        "\n",
        "# Visualize the results\n",
        "plt.scatter(data[:, 0], data[:, 1], c=clusters, cmap='viridis', alpha=0.7)\n",
        "plt.title('DBSCAN Clustering - Manual Implementation')\n",
        "plt.xlabel('X Axis')\n",
        "plt.ylabel('Y Axis')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCyP0WbPsZvC"
      },
      "source": [
        "# DBSCAN implementation using the sklearn library\n",
        "---\n",
        "\n",
        "Same as **K-Means**, you don't want to write large portions of code particually when it comes to process big and multidimensional datasets. Let's see how easy is to run **DBSCAN** in just a few steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJxU1ZTpk-Cs"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic spatial data for practice\n",
        "np.random.seed(42)\n",
        "x = np.random.rand(300) * 10\n",
        "y = np.random.rand(300) * 10\n",
        "data = pd.DataFrame({'X': x, 'Y': y})\n",
        "\n",
        "# Create GeoDataFrame from the synthetic data\n",
        "geometry = gpd.points_from_xy(data['X'], data['Y'])\n",
        "gdf = gpd.GeoDataFrame(data, geometry=geometry)\n",
        "\n",
        "# Visualize the synthetic data\n",
        "gdf.plot(marker='p', color='blue', figsize=(8, 8))\n",
        "plt.title('Synthetic Spatial Data')\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Y-axis')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neNHT42xuG9M"
      },
      "outputs": [],
      "source": [
        "# Implement DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.8, min_samples=5)\n",
        "gdf['dbscan_cluster'] = dbscan.fit_predict(gdf[['X', 'Y']])\n",
        "\n",
        "# Visualize DBSCAN clustering result\n",
        "gdf.plot(column='dbscan_cluster', categorical=True, legend=True, figsize=(8, 8), cmap='Set1')\n",
        "plt.title('DBSCAN Clustering Result - With sklearn')\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Y-axis')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GZSUrIczJt8"
      },
      "source": [
        "As you have noticed the key in **DBSCAN** is the definition of **eps(epsilon)** and the **min_samples**( minimum points).\n",
        "\n",
        "In a relative recent publicacion (1), authors suggests to use a larger **min_samples** for *large and noisy data sets*, and to adjust **eps** (epsilon) depending on whether you get too large clusters (decrease epsilon) or too much noise (increase epsilon).\n",
        "\n",
        "It is important you understand that ***clustering requires iterations***, so please take time to play with those values so you can see the difference on the outcomes.\n",
        "\n",
        "```\n",
        "(1) Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n",
        "DBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN.\n",
        "ACM Transactions on Database Systems (TODS), 42(3), 19.\n",
        "```\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ACXQMdAlOFR"
      },
      "source": [
        "Please address the following questions:\n",
        "\n",
        "1. What is the purpose of **`DBSCAN(eps=1.5, min_samples=8)`**?\n",
        "2. Have you tried to update/change the **nmin_sample** size?\n",
        "3. What does **`dbscan.fit_predict(gdf[['X', 'Y']])`** do?\n",
        "4. What does **`gdf.plot(column='dbscan_cluster', categorical=True, legend=True, cmap='Set1')`** do?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRVss-I8s2XE"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Write your responses here:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6sorqDhmEEL"
      },
      "source": [
        "# Challenges - Let's get your coding skills tested.\n",
        "\n",
        "I know that so far you have been running the code cells and getting most of the results. Now, it's time to do some thinking, try writing your own code, and correct some common mistakes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFo2kCn_1SMH"
      },
      "source": [
        "## Fix the code\n",
        "Please read carefully the following code cells and try to address the issues included in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a30xbr7ymLa_"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "x = np.random.rand('150') * 10\n",
        "y = np.random.rand('300') * 10\n",
        "data = pd.DataFrame({'X': x, 'Y': y})\n",
        "\n",
        "gdf.plot(marker='s', color='r', figsize=(8, 8))\n",
        "plt.title('Synthetic Spatial Data'):\n",
        "  plt.xlabel('X-axis')\n",
        "  plt.ylabel('Y-axis')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkHPNnAume1t"
      },
      "outputs": [],
      "source": [
        "# What would happen if you update the n_clusters from 3 to 4.\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "gdf['kmeans_cluster'] = kmeans.fit_predict(gdf[['X', 'Y']])\n",
        "\n",
        "# Check this link https://matplotlib.org/stable/users/explain/colors/colormaps.html\n",
        "# and try to make the plot with another cmap, instead of the viridis\n",
        "\n",
        "gdf.plot(column='dbscan_cluster', categorical=True, legend=True, figsize=(8, 8), cmap='viridis')\n",
        "plt.title('DBSCAN Clustering Result'):\n",
        "  plt.xlabel('X-axis')\n",
        "  plt.label('Y-axis')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VekjBIjioLhI"
      },
      "source": [
        "# Applying Spatial Clustering to real data\n",
        "\n",
        "Now, let's try to implement both clustering algoritms to analyze real data, related to reported crime in the UK. This data can be found in the following link https://data.police.uk/data/, you can select a range of time and then the force that you would like to get the data. For this example we will work with the reported crime from **Jan 2023 to Sep 2023** collected by the Metropolitan Police Service.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yaDFEMKCCzY"
      },
      "source": [
        "Go to Moodle and download the **crime_data_2023_london** file. Unzip the file, then upload it to your Google Drive so you can access it from this notebook. At this stage, you should already know how to find the path to any uploaded file and use it in your notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpwI37WE8uFG"
      },
      "source": [
        "## K-Means & DBSCAN - Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQVPDAEmKeHs"
      },
      "source": [
        "## Reading the Data\n",
        "\n",
        "Initally we need to read the data., make sure you upload the data that was provided in Moodle in your Google drive so you can update the path below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OU4nc1IHxiYP"
      },
      "outputs": [],
      "source": [
        "# Load crime data for London\n",
        "crime_data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/CrimeLondon/Crime/london_crime_2023.csv\", dtype={\"Location\": str, \"LSOA code\": str, \"LSOA name\": str, \"Crime type\": str })\n",
        "crime_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfNW7g3yKix4"
      },
      "source": [
        "## Pre-Processing\n",
        "\n",
        "We always want to check the size of the data we load in memory, as `crime_data`, you see you have loaded more than 96.000 rows and 12 columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud5D3cyMl-oK"
      },
      "outputs": [],
      "source": [
        "crime_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gslG5oOTLIuk"
      },
      "source": [
        "There are a lot of columns in this dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhKnCMp2yF4q"
      },
      "outputs": [],
      "source": [
        "crime_data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69o3FHPPLOMi"
      },
      "source": [
        "Let's keep only specific columns that we care about:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnKOT3kkyKKX"
      },
      "outputs": [],
      "source": [
        "keep_cols = [\n",
        "    \"Month\",\n",
        "    \"Latitude\",\n",
        "    \"Longitude\",\n",
        "    \"Location\",\n",
        "    \"LSOA code\",\n",
        "    \"LSOA name\",\n",
        "    \"Crime type\",\n",
        "]\n",
        "crime_data = crime_data[keep_cols]\n",
        "crime_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNxZOtf61by4"
      },
      "source": [
        "## Removing Null values\n",
        "\n",
        "Before processing this data any further, we need to make sure we don't have any **Null** rows in your **Latitude** and **Longitude** columns. A very easy way to do it, is by using the isnull() function and using **mean** to calculate the fraction of records in **crime_data** where either the latitude or longitude is missing. This combines the two Boolean Series element-wise, returning True if either latitude or longitude is missing in that row.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLR-Jftuyrhh"
      },
      "outputs": [],
      "source": [
        "(crime_data[\"Latitude\"].isnull() | crime_data[\"Longitude\"].isnull()).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTjhCNmg6q76"
      },
      "source": [
        "When applied to a Boolean Series, True is treated as 1 and False as 0.\n",
        "So taking the mean gives you the proportion (fraction) of rows that have a missing latitude or longitude."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVO93kRlL5uQ"
      },
      "source": [
        "Removing the Null rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_CtXqVMyzjn"
      },
      "outputs": [],
      "source": [
        "crime_data = crime_data[(crime_data[\"Longitude\"].notnull() & crime_data[\"Latitude\"].notnull())]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KYwuL5GL8ap"
      },
      "source": [
        "Try again the mean is Zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hZ7Ywl6_QAi"
      },
      "outputs": [],
      "source": [
        "(crime_data[\"Latitude\"].isnull() | crime_data[\"Longitude\"].isnull()).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQaKg8SAzL3V"
      },
      "outputs": [],
      "source": [
        "crime_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAxEfSot8iIW"
      },
      "source": [
        "Ok, now the data is clean, and we make sure there are no gaps in the attributes we need to geocode this table and transform it into a GeoPandas DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bxkf5NzMBNK"
      },
      "source": [
        "## Construct the GeoDataFrame\n",
        "\n",
        "Now let's construct a GeoDataFrame from this data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JplMUWz2rs9"
      },
      "outputs": [],
      "source": [
        "geometry = gpd.points_from_xy(crime_data.Longitude, crime_data.Latitude)\n",
        "gdf_crime = gpd.GeoDataFrame(crime_data, geometry=geometry, crs=\"EPSG:4326\")\n",
        "gdf_crime.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pXJ3nuN8w9q"
      },
      "source": [
        "you see you have now a new column called **Geometry**, and the table is now spatial, including a defintion of of coordinate system. **EPSG:4326** which is the standard code for the **WGS84 coordinate system**, which is used globally for Latitude and Longitude coordinates (like those from GPS). Now is ready to make a map.\n",
        "\n",
        "This is a relative large dataset, if we use the tradional `.plot()` or `explore()` we migth face a huge delay. That is why we installed **lonboard** to represent large datasets like this one. A very simple way to make this map is using `viz(name_of_geodataframe)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i2XXURq9GHS"
      },
      "outputs": [],
      "source": [
        "viz(gdf_crime)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM_HLo-A-zGI"
      },
      "source": [
        "You may have noticed that even though the dataset is supposed to be about car accidents in **London**, there are points scattered across the UK. Therefore, we still need to perform additional data cleaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0qmPFxsMHMv"
      },
      "source": [
        "## More cleaning...we need also a Spatial filter\n",
        "\n",
        "We can check the boundaries of this dataset, with the function `total_bounds`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_zZqfQI82mJ"
      },
      "outputs": [],
      "source": [
        "gdf_crime.total_bounds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-IpX6CU_MLo"
      },
      "source": [
        "This is clearly not the spatial boundaries of london."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3sW8ldgMOJy"
      },
      "source": [
        "By using the tool https://norbertrenner.de/osm/bbox.html to get the precise boundary box you need to spatially filter your analysis and remove any outlier rows that are not located in the area of interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5neMcB0885Wd"
      },
      "outputs": [],
      "source": [
        "ld_bbox = [-0.591,51.285,0.381,51.678] # London B. Box"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yDNe7e5Ml-i"
      },
      "source": [
        "Slicing the dataset based on the new boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty1JTaLj9InB"
      },
      "outputs": [],
      "source": [
        "gdf_crime = gdf_crime[gdf_crime.intersects(shapely.box(*ld_bbox))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYRVEFu2Mq1g"
      },
      "source": [
        "By using `.shape` we can check the new amount of rows in the spatial dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqeDg1Wh9Ove"
      },
      "outputs": [],
      "source": [
        "gdf_crime.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JNrbx7L3RvI"
      },
      "source": [
        "Now we can use again the [lonboard](https://developmentseed.org/lonboard/latest/) library to valide if our spatial filter worked.\n",
        "\n",
        "\n",
        "There are two methods in this very recent and under development library `viz(gdf)` which as we could see before is a quick method to plot your big datasets.\n",
        "\n",
        "The second one where we have more control of how the layer would look like `ScatterplotLayer`, thus we can create a `layer` then use other methods in the library to customize the visualization of the layer, you could also load several layers, including lines and polygons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtjvGrEN5N2Q"
      },
      "outputs": [],
      "source": [
        "viz(gdf_crime)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf80EXeAArMF"
      },
      "source": [
        "Now lets try the second method to plot a map using lonboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdPlU5SV8DN2"
      },
      "outputs": [],
      "source": [
        "layer = ScatterplotLayer.from_geopandas(gdf_crime)\n",
        "map = Map(layers=[layer], _height=500)\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j-AyUD2AQO0"
      },
      "source": [
        "**Did you get an error here? What do you think is missing?**\n",
        "\n",
        "**Do we need to import something important that we haven't imported yet?,**\n",
        "\n",
        "Have a look at the library documentation website https://developmentseed.org/lonboard/latest/examples/internet-speeds/#dependencies and see if you can identify the **import** we are missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFROfyXJBFnJ"
      },
      "outputs": [],
      "source": [
        "#Add the Fixed code in here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8JcweLxIRD-"
      },
      "source": [
        "## Customizing the Layer\n",
        "\n",
        "Now we can customize the previous layer by plotting by `Crime type` in an initial attemtp to unhide any spatial pattern. Let's initially check how many catgeories we have in this `Crime type` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKUcl2t9-jGM"
      },
      "outputs": [],
      "source": [
        "crime_type = gdf_crime['Crime type'].value_counts()\n",
        "crime_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_lfgU7fIMD6"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "gdf_crime.groupby('Crime type').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDGMgFmHI3Rh"
      },
      "source": [
        "We can see that \"Violence and sexual offences\" is the **crime type** with the most reports. Now, let's check for any spatial patterns by plotting these categories on our previously loaded map.\n",
        "\n",
        "To do this, we need to assign a color to each crime type, which we can accomplish by creating an array of all the unique categories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3YfexCV-7ek"
      },
      "outputs": [],
      "source": [
        "categories = gdf_crime['Crime type'].unique()\n",
        "colors = sns.color_palette(\"bright\", len(categories))\n",
        "colors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0KyeXrUJVF2"
      },
      "source": [
        "Now using the same principle, we can use Numpy to create a matrix that group every `Crime type` with its correspondant `color code. ` This code will use the coding `RGBA(Red, Green, Blue,Alpha)` the last value corresponde to transparency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXz19VdL_YLA"
      },
      "outputs": [],
      "source": [
        "# Get unique categories\n",
        "categories = gdf_crime['Crime type'].unique()\n",
        "\n",
        "# colour seaborn's \"tab10\" color palette\n",
        "colors = sns.color_palette(\"bright\", len(categories))\n",
        "\n",
        "# Create a dictionary to map categories to colors\n",
        "color_dict = dict(zip(categories, colors))\n",
        "\n",
        "color_array = np.array([tuple(np.append(np.multiply(color_dict.get(x, (0, 0, 0)), 255).astype(int), 255)) for x in gdf_crime['Crime type']], dtype=np.uint8)\n",
        "color_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owfNBVjyJ9OG"
      },
      "source": [
        "With this new array containing the color codes for each crime type category, you can now use the layer's propertiesâ€”such as `radius_scale`, `opacity`, and `get_fill_color`â€”to customize the map.\n",
        "\n",
        "**Run the following code, and return to the map you previously plotted (the one where you have corrected the code) to view the updated, color-coded layer. You probably need to zoom it to see the points associated to the Crime Type Categories**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQq13bNL_lsW"
      },
      "outputs": [],
      "source": [
        "layer.radius_scale = 40\n",
        "layer.opacity = 0.05\n",
        "layer.get_fill_color = color_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXr_T6NkKlr7"
      },
      "source": [
        "## Density Surface - HeatMap\n",
        "\n",
        "As you may have noticed, even though a choropleth map provides some insight, it can still be difficult to detect clear spatial patterns.\n",
        "\n",
        "Let's try using the heatmap option from Geopandas to better visualize the density and distribution of our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUvbiIs4LOkk"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium import plugins\n",
        "\n",
        "map = folium.Map(location=[51.518591, -0.108447], tiles=\"Cartodb dark_matter\", zoom_start=9)\n",
        "\n",
        "heat_data = [[point.xy[1][0], point.xy[0][0]] for point in gdf_crime.geometry]\n",
        "plugins.HeatMap(heat_data).add_to(map)\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbaabS0iMAWO"
      },
      "source": [
        "**Do you think the previous HeatMap is some how useful?**\n",
        "\n",
        "**Can you unveail any hidden spatial pattern?**\n",
        "\n",
        "I guess your reply is no.\n",
        "\n",
        "**HeatMaps** are useful as initial step to explore any dense areas based on the location of your dataset, but still does not provide any relevant insigth, as we saw in the previous lecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi0kpD5UMzOe"
      },
      "source": [
        "## Reducing the Subjectivity of the Map - Spatial Clustering\n",
        "\n",
        "Now we can implement **K-Means** and **DBSCAN** by using the sklearn library, and then plot the results using a popular library called Plotly Express - https://plotly.com/python/plotly-express/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGJ8JNXGuDd3"
      },
      "outputs": [],
      "source": [
        "# Let's start with KMeans.\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "gdf_crime['kmeans_cluster'] = kmeans.fit_predict(gdf_crime[['Longitude', 'Latitude']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gv4gUszZ_YJ"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig_kmeans = px.scatter_mapbox(\n",
        "    gdf_crime,\n",
        "    lat=\"Latitude\",\n",
        "    lon=\"Longitude\",\n",
        "    color=\"kmeans_cluster\",\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    zoom=9,  # Adjust zoom level as needed\n",
        "    title=\"K-means Clustering\",\n",
        "    height=700,    # Set initial height in pixels (width is responsive)\n",
        "    opacity=0.5,\n",
        "\n",
        ")\n",
        "fig_kmeans.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VyU9SFr2xmg"
      },
      "source": [
        "Now, to make the spatial clustering with DBSCAN, there is a fundamental step to make it correctly. Do you recall what is the crs defined in `gdf_crime`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5PcBSJY3zy-"
      },
      "outputs": [],
      "source": [
        "print(gdf_crime.crs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hknc-kXL4BpQ"
      },
      "source": [
        "You should have seen **EPSG:4326**, which is a geographic coordinate system. That means you have your data using Latitude and Longitude.\n",
        "\n",
        "Can you calculate distances using Latitude and Longitude, like the **eps (epsilon)** needed in the DBSCAN implementation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMs_Q4OY4xev"
      },
      "source": [
        "As I really hope your reply was **NO**. Then we need to reproject our geopandas dataframe and transform it to use a local and projectec reference system. The EPSG code for the UK is **EPSG:27700**, which represents the **British National Grid (BNG)**. This coordinate system is used for onshore Great Britain and the Isle of Man, and it is based on the OSGB36 geodetic datum. https://epsg.io/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P39RBETJ3zv8"
      },
      "outputs": [],
      "source": [
        "gdf_crime_projected = gdf_crime.to_crs(epsg=27700)\n",
        "gdf_crime_projected.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdW4Za0N7Etc"
      },
      "source": [
        " **DBSCAN** use the X and Y coordinates of your dataset, but we don't have those columns, so we need to use the `geometry` column to extract these X/Y coordinates for DBSCAN:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLlo_GPD7evJ"
      },
      "outputs": [],
      "source": [
        "gdf_crime_projected['geometry_x'] = gdf_crime_projected.geometry.x\n",
        "gdf_crime_projected['geometry_y'] = gdf_crime_projected.geometry.y\n",
        "gdf_crime_projected.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3-IHhh05UWq"
      },
      "source": [
        "Now we can define the most relevant parameters of the DBSCAN implementation.\n",
        "\n",
        "*   meters_eps = 200 (200 meters)\n",
        "*   min_samples_val = 40 (Min of 40 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGtnqb1B6Dfl"
      },
      "outputs": [],
      "source": [
        "meters_eps = 200\n",
        "min_samples_val = 40\n",
        "\n",
        "dbscan = DBSCAN(eps=meters_eps, min_samples=min_samples_val)\n",
        "\n",
        "gdf_crime_projected['dbscan_cluster'] = dbscan.fit_predict(\n",
        "    gdf_crime_projected[['geometry_x', 'geometry_y']])\n",
        "\n",
        "gdf_crime_projected.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaixDGo686Uu"
      },
      "source": [
        "Let's map the final results, using Lat and Long columns (requiered by plotly express)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPdyTlb27_Ab"
      },
      "outputs": [],
      "source": [
        "fig_dbscan = px.scatter_mapbox(\n",
        "    gdf_crime_projected,\n",
        "    lat=\"Latitude\",\n",
        "    lon=\"Longitude\",\n",
        "    color=\"dbscan_cluster\",\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    zoom=9,  # Adjust zoom level as needed\n",
        "    title=\"DBSCAN Clustering\",\n",
        "    height=700,    # Set initial height in pixels (width is responsive)\n",
        "    opacity=0.5,\n",
        "\n",
        ")\n",
        "fig_dbscan.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_DIb1spNbF5"
      },
      "source": [
        "Now we can start adjusting the **required parameters** to perform multiple evaluations.\n",
        "\n",
        "**Finally:** Take some time **to adjust** **and test** **several values** for both **KMeans** and **DBSCAN** and observe how the spatial clustering changes.\n",
        "\n",
        "This way, you are minimising the **subjectivity of your map** by including spatial properties of your data; still, you need to work with meaningful arguments to define the key parameters. That is why â€”**there is no such thing as removing the subjectivity of maps**-now, you can reduce it by implementing a **machine learning technique** into large datasets to reveal hidden patterns.\n",
        "\n",
        "Recall this clustering include only the location of the reported crimes. You can eventually also create clusters by using a **categorial attribute** like **type of crime** and see if there is any spatial correlation with areas where those crimes were reported."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EZd_QkOVCPE"
      },
      "source": [
        "# Conclusions\n",
        "\n",
        "Our exploration into spatial clustering using the **KMeans** and **DBSCAN** algorithms has provided valuable insights into uncovering patterns within crime data in London. The visualizations created with `Plotly` and `GeoPandas` have allowed us to observe how these algorithms group spatial data points based on their geographical coordinates.\n",
        "\n",
        "This brief introduction to spatial clustering offers just a glimpse into the world of data-driven spatial analysis. The use of **KMeans**, a partitioning and unsupervised clustering method, and **DBSCAN**, a density-based clustering approach, also unsupervised method,  has display distinct patterns within the crime dataset. We've witnessed the power of these algorithms in identifying clusters and anomalies, offering a foundation for understanding spatial distributions. It was also relevant to see how useful using external and powerful Machine Leaning libraries like `Sklearn` can be in python, and how much  code would eventually save us.\n",
        "\n",
        "**It is important to note that spatial clustering extends far beyond the methods explored here. Statistical and spatial clustering techniques provide a diverse toolkit for researchers and analysts, enabling a deeper understanding of underlying patterns.**\n",
        "\n",
        "**If you want to continue your learning and practice into spatial data analysis, in the advance module (GG4257)** I will cover more libaries, how to install locally the python stack, and we will cover more methods that go beyond the geographic proximity, incorporating statistical measures and advanced clustering algorithms.\n",
        "\n",
        "As final step, now is your turn to try some of these code lines with the **Exercise notebook available in Moodle.**\n",
        "\n",
        "**Aditional Note:** The library we used called lonboard https://developmentseed.org/lonboard/latest/ and you have installed at the beinging of this notebook allow us to load much larger datasets, around 2 to 4 millons of records, something that would be nearly impossible to do it with the tradtional GIS tools and your computer as we will overpass the computational capacity in a matter of seconds.\n",
        "\n",
        "Here in Google Colab you also have some **RAM and Disk limits**, so you probably will need to be aware of how you manipulate load big datasets in memory.\n",
        "\n",
        "In general terms we can do plenty of analysis here, but unless we are willing to set up a credit card and pay for more cloud resources, the best way to have **full control and use more RAM or Disk resources** is installing the so-called python stack in our machines, so we can use our local resources without paying anything extra, the limit will be defined by the RAM and Disk limits on your machine."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "py4sa",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "a62a218f45948969006c944db2ca1c519af623da5e08f864ae6aafcacb945df1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
